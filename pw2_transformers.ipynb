{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4JEFb7B0FwE"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PaulLerner/aivancity_nlp/blob/main/pw2_transformers.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYgvImx0FwK"
      },
      "source": [
        "# Make a group for the Homework\n",
        "\n",
        "Before starting this Practical Work, make sure that you have a group (3 students) for the Homework.\n",
        "\n",
        "If you have trouble finding a group, please tell the teacher. If you do the homework alone without authorization, you will get 0/20.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xa3peaE0FwK"
      },
      "source": [
        "# Installation and imports\n",
        "\n",
        "Hit `Ctrl+S` to save a copy of the Colab notebook to your drive\n",
        "\n",
        "Run on Google Colab GPU:\n",
        "- Connect\n",
        "- Modify execution\n",
        "- GPU\n",
        "\n",
        "![image.png](https://paullerner.github.io/aivancity_nlp/_static/colab_gpu.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwGlb32Z0FwL"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGesv_720FwN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGG31qMS0FwN"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available(), \"Connect to GPU and try again (ask teacher for help)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe4ha9NgfGy"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYj_37iZgjo9"
      },
      "source": [
        "Attention is a crucial component in the transformer, it allows to capture dependencies between different positions of two sequence of elements. In our case, and in most cases in NLP applications, sequences are sentences and elements are (sub)words.\n",
        "It is a powerful operation that allows to learn an alignment between each element in two sequences. It generates a score of how related each element in sequence1 and sequence2 are between each other.\n",
        "Understanding how attention works and being able to implement it are essential for anyone working with transformers.\n",
        "\n",
        "Given a query ($Q$), key ($K$), and value ($V$) tensors, the attention mechanism computes a weighted sum of the value tensor based on the similarity between the query and key tensors as shown in the following equation:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big)V\n",
        "$$\n",
        "\n",
        "where\n",
        "- $Q$ represents the query tensor.\n",
        "- $K$ represents the key tensor.\n",
        "- $V$ represents the value tensor.\n",
        "- $d_k$ represents the dimensionality of the key tensor.\n",
        "\n",
        "This is the image that was in the [original Transformer paper](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) and that shows the computations used in the attention.\n",
        "\n",
        "Forget about the right part, we'll get back to that later in the lab.\n",
        "\n",
        "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)\n",
        "\n",
        "\n",
        "In this exercise, we will dive into the attention mechanism. To do so, we are going to build a simple cross-attention function that we will then extend to a more complex multi-head self-attention module that incorporates the concept of causality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0lBpBvtiEQX"
      },
      "source": [
        "## Building a Simple Self-Attention Function\n",
        "\n",
        "In self-attention, a single sequence acts as the query $Q$, key $K$, and value $V$, allowing attention to be computed within the sequence itself. This can be useful for syntactic where an attention head can model the relationship between part of speech like subjects and verbs.\n",
        "\n",
        "\n",
        "Given an input sequence $S$ and the transformation weights $W_Q$, $W_K$ and $W_V$, complete the `self_attention` function in the cell below.\n",
        "\n",
        "You need to implement the following:\n",
        "- Calculate the query, key, and value projections using linear transformations.\n",
        "- Compute the attention scores by performing the dot product between the query and key tensors.\n",
        "- Apply softmax activation to the attention scores to obtain the attention weights.\n",
        "- Multiply the attention weights with the value tensor to get the attended values.\n",
        "- Return the attended values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-iITZz0FwQ"
      },
      "source": [
        "Hint: Matrix sizes\n",
        "\n",
        "- q: query size\n",
        "- d: hidden dimension\n",
        "- c: context length\n",
        "\n",
        "\n",
        "- Q: 1xqxd\n",
        "- K, V: 1xcxd\n",
        "- Q x K:  1xqxd x 1xsxd.T\n",
        "- (QK) x V:  1xqxs  x  V  1xsxd\n",
        "\n",
        "  \n",
        "- Attn: 1xqxd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAx2Brb80FwR"
      },
      "outputs": [],
      "source": [
        "def self_attention(S, W_Q, W_K, W_V):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hak9f70I0FwR",
        "outputId": "136859f0-adbd-4b22-ff80-e0c09e149047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Shape: torch.Size([1, 13, 2])\n"
          ]
        }
      ],
      "source": [
        "# Sequence\n",
        "S = torch.rand((1,13,3))\n",
        "\n",
        "# Projections\n",
        "W_Q = torch.rand((3, 2))  # Query weights\n",
        "W_K = torch.rand((3, 2))  # Key weights\n",
        "W_V = torch.rand((3, 2))  # Value weights\n",
        "\n",
        "# Perform self-attention\n",
        "attended_values = self_attention(S, W_Q, W_K, W_V)\n",
        "\n",
        "# Expected output # 1,13, 2 (B, Sequence, Projection)\n",
        "print(f\"Output Shape: {attended_values.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZyqDvxt0FwS"
      },
      "source": [
        "## Multi-Head\n",
        "\n",
        "However, the relations present even in a single sentence are more than one. Think about number and gender agreement as one, the semantic relation between subject and object, the functional aspect that verb arguments have etc. All this cannot be modeled by a single head.\n",
        "\n",
        "For this reason, we are going to extend the single-head attention function to **multi-head attention**. In the previous implementation, we had one set of weights for the input query, resulting in a single type of _relationship between the the source and target sequence_. With multi-head attention, we can utilize _multiple parallel single-head attention modules_ to obtain diverse relationships between the query and the values. The attention operation works by projecting the sequences through a multiplication with a projection matrix, and then computing the alignment score. These are are all operation that can be parallelized since there's no interdependency between each each head. For this reasons, each head could learn to model a different linguistic intereation useful for many downstream tasks, be it syntactic, semantic or generation-based..\n",
        "\n",
        "\n",
        "As we've seen in class, this can be done simply by reshaping queries, keys and values.\n",
        "\n",
        "Project back the results using $W_O$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUJ8omOG0FwT"
      },
      "outputs": [],
      "source": [
        "def multi_head_attention(S, W_Q, W_K, W_V, W_O, num_heads=4):\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-1NwhOc0FwT",
        "outputId": "34059f4b-a470-4848-e286-b1a08b3d5e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Shape: torch.Size([1, 13, 8])\n"
          ]
        }
      ],
      "source": [
        "# Sequence\n",
        "S = torch.rand((1,13,8))\n",
        "\n",
        "# Projections\n",
        "W_Q = torch.rand((8, 8))  # Query weights\n",
        "W_K = torch.rand((8, 8))  # Key weights\n",
        "W_V = torch.rand((8, 8))  # Value weights\n",
        "W_O = torch.rand((8, 8))  # Output proj\n",
        "\n",
        "# Perform self-attention\n",
        "attended_values = multi_head_attention(S, W_Q, W_K, W_V, W_O)\n",
        "\n",
        "# Expected output # 1, 13, 8 (B, Sequence, Projection)\n",
        "print(f\"Output Shape: {attended_values.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZEpOhX70FwT"
      },
      "source": [
        "## Causal Mask\n",
        "\n",
        "GPT uses a version of self-attention called causal self-attention. When training our models for tasks like language modeling and machine translation, in practice we feed the entire train sequence to the model but, at every timestep, we want to prevent it to compute the alignment with future tokens. For this reason we use a mask that we incrementally lift at every timestep. For instance, we have a sentence that says \"Libson is a great city to live in\". At time 0, we feed the entire sentence to the model masking everything but the first token. Using the strikethrough format as masking, this will be what the model sees at step 0:\n",
        "\n",
        "- Time 0: Libson ~is a great city to live in~\n",
        "\n",
        "We then let the model generate a token a and move to step 1 where we are masking everything but the first two tokens\n",
        "\n",
        "- Time 1: Libson is ~a great city to live in~\n",
        "\n",
        "and so on...\n",
        "\n",
        "- Time 2: Libson is a ~great city to live in~\n",
        "- Time 3: Libson is a great ~city to live in~\n",
        "- Time 4: Libson is a great city ~to live in~\n",
        "- Time 5: Libson is a great city to ~live in~\n",
        "- Time 6: Libson is a great city to live ~in~\n",
        "\n",
        "\n",
        "![transformer](https://paullerner.github.io/aivancity_nlp/_static/attention_mask.png)\n",
        "\n",
        "Apply mask on attention using `torch.tril` and `masked_fill`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16Osxxai0FwT"
      },
      "outputs": [],
      "source": [
        "def causal_multi_head_attention(S, W_Q, W_K, W_V, W_O, num_heads=4):\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A758v1Xd0FwU",
        "outputId": "94783d2e-8dbe-4a5b-9c63-6ec1b64d406f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bidirectional attention:\n",
            "tensor([[[[ 7.6750,  9.8763,  7.8718],\n",
            "          [11.7096, 14.9262, 11.9657],\n",
            "          [ 8.8149, 11.3644,  9.0476]],\n",
            "\n",
            "         [[ 6.9848,  8.4330,  6.1530],\n",
            "          [ 8.9807, 10.8143,  7.9368],\n",
            "          [ 7.9229,  9.5573,  6.9868]],\n",
            "\n",
            "         [[ 8.4009, 10.4845,  9.2640],\n",
            "          [10.3813, 12.9258, 11.4504],\n",
            "          [ 8.6193, 10.7093,  9.5088]],\n",
            "\n",
            "         [[ 9.4824, 11.5997, 10.2568],\n",
            "          [13.4200, 16.4683, 14.5326],\n",
            "          [10.5152, 12.8738, 11.3774]]]])\n",
            "\n",
            "causal attention:\n",
            "tensor([[[[ 7.6750,    -inf,    -inf],\n",
            "          [11.7096, 14.9262,    -inf],\n",
            "          [ 8.8149, 11.3644,  9.0476]],\n",
            "\n",
            "         [[ 6.9848,    -inf,    -inf],\n",
            "          [ 8.9807, 10.8143,    -inf],\n",
            "          [ 7.9229,  9.5573,  6.9868]],\n",
            "\n",
            "         [[ 8.4009,    -inf,    -inf],\n",
            "          [10.3813, 12.9258,    -inf],\n",
            "          [ 8.6193, 10.7093,  9.5088]],\n",
            "\n",
            "         [[ 9.4824,    -inf,    -inf],\n",
            "          [13.4200, 16.4683,    -inf],\n",
            "          [10.5152, 12.8738, 11.3774]]]])\n",
            "Output Shape: torch.Size([1, 3, 8])\n"
          ]
        }
      ],
      "source": [
        "# Sequence\n",
        "S = torch.rand((1,3,8))\n",
        "\n",
        "# Projections\n",
        "W_Q = torch.rand((8, 8))  # Query weights\n",
        "W_K = torch.rand((8, 8))  # Key weights\n",
        "W_V = torch.rand((8, 8))  # Value weights\n",
        "W_O = torch.rand((8, 8))  # Output proj\n",
        "\n",
        "# Perform self-attention\n",
        "attended_values = causal_multi_head_attention(S, W_Q, W_K, W_V, W_O)\n",
        "\n",
        "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
        "print(f\"Output Shape: {attended_values.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9-ZOxnl0FwU"
      },
      "source": [
        "\n",
        "We can now look back at the attention figure from the paper. Hopefully, you are now able to understand also the right side of the figure.\n",
        "\n",
        "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs7ar9XJu0u6"
      },
      "source": [
        "## Pytorch Module\n",
        "\n",
        "The last modification involves embedding our function into a PyTorch module. As you may have noticed, in the previous exercise, we passed the transformation weights as inputs to the function. In a real-world scenario, these matrices are learned, and PyTorch can keep track of them for us.\n",
        "\n",
        "- Complete the missing lines on the initialization of the module and the forward pass.\n",
        "- add dropout on the attention weights and the output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh-3sFiA0FwV"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Por_QTl20FwV"
      },
      "outputs": [],
      "source": [
        "attention_module = CausalSelfAttention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf7Smtff0FwV",
        "outputId": "477bc636-0911-4a1e-e4bd-580536300606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Shape: torch.Size([1, 3, 8])\n"
          ]
        }
      ],
      "source": [
        "# Sequence\n",
        "S = torch.rand((1,3,8))\n",
        "\n",
        "# Perform self-attention\n",
        "attended_values = attention_module(S)\n",
        "\n",
        "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
        "print(f\"Output Shape: {attended_values.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABD9dGfH0FwV"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "\n",
        "![transformer](https://paullerner.github.io/aivancity_nlp/_static/transformer_decoder.png)\n",
        "\n",
        "## Attention is almost all you need: feedforward neural network\n",
        "\n",
        "Simple Neural network of two layers with a ReLU activation in-between and dropout at output. The intermediate dimension should be 4 times `hidden_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmnPe9QI0FwW"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWA_KI0U0FwW"
      },
      "source": [
        "## Transformer Block\n",
        "\n",
        "- stack CausalSelfAttention and FeedForward\n",
        "- add residual connections\n",
        "- add layer norms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUk4JxsS0FwW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFzyc0xQ0FwW"
      },
      "outputs": [],
      "source": [
        "# Sequence\n",
        "S = torch.rand((1,3,8))\n",
        "\n",
        "block = Block()\n",
        "# Perform self-attention\n",
        "output = block(S)\n",
        "\n",
        "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
        "print(f\"Output Shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWfLbMv10FwW"
      },
      "source": [
        "## Complete Transformer\n",
        "- word embeddings\n",
        "- position embeddings\n",
        "- as many blocks as you like to stack\n",
        "- output layer back to the vocabulary (no need for softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7clbCSPH0FwW"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size=100, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD7zYc5Y0FwX"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhGsv1GK0FwX"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.randint(0,100,(1, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_YdLDss0FwX",
        "outputId": "d05e55bd-a579-40d4-a82c-063e11602cd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[85, 10,  4]])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cTOCYRD0FwX"
      },
      "outputs": [],
      "source": [
        "logits = transformer(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NeCE0mC0FwX",
        "outputId": "b2d8dfef-42b8-412b-c3e0-9b9040fc849d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 100])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# scores (not probabilities because not normalized) over the complete vocabulary, for each token in the sentence\n",
        "# shape: batch size, seq_len, V\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRiFro12JhO3"
      },
      "source": [
        "# Training\n",
        "\n",
        "A peak into Language Modeling (next class)\n",
        "\n",
        "\n",
        "![lm](https://paullerner.github.io/aivancity_nlp/_static/lm.png)\n",
        "\n",
        "A language model estimates the probability of a sequence of words $w$:\n",
        "$$P(w)=\\prod_t^{|w|} P(w_t | w_{<t}) = P(w_1)  P(w_2|w_1)  P(w_3 | w_1 w_2)...$$\n",
        "\n",
        "See how this turns into a sequence of classification problem:\n",
        "- first $P(w_1)$\n",
        "- then $P(w_2|w_1)$\n",
        "- etc.\n",
        "\n",
        "The model \"predicts the next word\" given a context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBk5Xeuz0FwY"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suflbC7O0FwY",
        "outputId": "a68950db-8054-4d8f-f215-4fdf61a0ec28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paul/anaconda3/envs/matos/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test 4358\n",
            "train 1801350\n",
            "validation 3760\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
        "\n",
        "dataset = {k: v[\"text\"] for k, v in dataset.items()}\n",
        "\n",
        "for k, v in dataset.items():\n",
        "    print(k, len(v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwvXjZuX0FwY",
        "outputId": "7ee59534-3ffd-48ea-dfb2-141d6df1ebd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhY5qepG0Fwe"
      },
      "source": [
        "## tokenization\n",
        "\n",
        "We almost did not talk about tokenization yet! We've assumed words, which is impractical given finite vocabulary size.\n",
        "\n",
        "Instead, LLMs rely on BPE, a data compression technique, which segments rare words into subwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0melMPH0Fwf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhMET4a0Fwf",
        "outputId": "a7ad1cc5-6ec6-4507-f268-08c751139222"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paul/anaconda3/envs/matos/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "seq_len=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Fw15260Fwf",
        "outputId": "78a98923-996f-4a39-9c67-77414e40c28b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ĠSen', 'j', 'Åį', 'Ġno', 'ĠV', 'alky', 'ria', 'Ġ3', 'Ġ:', 'ĠUn', 'recorded', 'ĠChronicles', 'Ġ(', 'ĠJapanese', 'Ġ:', 'Ġæ', 'Ī', '¦', 'å', 'ł', '´', 'ãģ®', 'ãĥ´ãĤ¡', 'ãĥ«', 'ãĤŃ', 'ãĥ¥', 'ãĥª', 'ãĤ¢', '3', 'Ġ,', 'Ġlit', 'Ġ.', 'ĠV', 'alky', 'ria', 'Ġof', 'Ġthe', 'ĠBattlefield', 'Ġ3', 'Ġ)', 'Ġ,', 'Ġcommonly', 'Ġreferred', 'Ġto', 'Ġas', 'ĠV', 'alky', 'ria', 'ĠChronicles', 'ĠIII', 'Ġoutside', 'ĠJapan', 'Ġ,', 'Ġis', 'Ġa', 'Ġtactical', 'Ġrole', 'Ġ@', '-', '@', 'Ġplaying', 'Ġvideo', 'Ġgame', 'Ġdeveloped', 'Ġby', 'ĠSega', 'Ġand', 'ĠMedia', '.', 'Vision', 'Ġfor', 'Ġthe', 'ĠPlayStation', 'ĠPortable', 'Ġ.', 'ĠReleased', 'Ġin', 'ĠJanuary', 'Ġ2011', 'Ġin', 'ĠJapan', 'Ġ,', 'Ġit', 'Ġis', 'Ġthe', 'Ġthird', 'Ġgame', 'Ġin', 'Ġthe', 'ĠV', 'alky', 'ria', 'Ġseries', 'Ġ.', 'ĠEmploy', 'ing', 'Ġthe', 'Ġsame', 'Ġfusion', 'Ġof', 'Ġtactical', 'Ġand', 'Ġreal', 'Ġ@', '-', '@', 'Ġtime', 'Ġgameplay', 'Ġas', 'Ġits', 'Ġpredecessors', 'Ġ,', 'Ġthe', 'Ġstory', 'Ġruns', 'Ġparallel', 'Ġto', 'Ġthe', 'Ġfirst', 'Ġgame', 'Ġand', 'Ġfollows', 'Ġthe', 'Ġ\"', 'ĠNam', 'eless', 'Ġ\"', 'Ġ,', 'Ġa', 'Ġpenal', 'Ġmilitary', 'Ġunit', 'Ġserving', 'Ġthe', 'Ġnation', 'Ġof', 'ĠGall', 'ia', 'Ġduring', 'Ġthe', 'ĠSecond', 'ĠEuro', 'pan', 'ĠWar', 'Ġwho', 'Ġperform', 'Ġsecret', 'Ġblack', 'Ġoperations', 'Ġand', 'Ġare', 'Ġpitted', 'Ġagainst', 'Ġthe', 'ĠImperial', 'Ġunit', 'Ġ\"', 'ĠCal', 'am', 'at', 'y', 'ĠRaven', 'Ġ\"', 'Ġ.', 'Ġ', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.tokenize(dataset[\"train\"][3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cxBv5qV0Fwf"
      },
      "outputs": [],
      "source": [
        "text_batch = texts[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpI5xCre0Fwf"
      },
      "source": [
        "huggingface's `transformers` provides a convenient way to tokenize text, it also takes care of padding the text so that we can wrap all examples of a batch in the same `Tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4cgKU950Fwf"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEZmM4eI0Fwg",
        "outputId": "a4ec5fb8-bbf4-4af1-8dbc-b2810561a721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[14207,   262,  1918,   286,  4463,   272,   837, 24233,   286,  9329,\n",
              "          4468,   283,   710,   837,   978,    71,  8310,   342,   286,  1024,\n",
              "          8704,   837,   287, 29771,   351,  5187, 39193,   286,  1971,   837,\n",
              "          2449,   346,  4835,   286, 18622,  8044,   290,  1854,   837,   547,\n",
              "          5295,   284, 20999, 30317, 16115,   284,  3896,   287,  7075,   286,\n",
              "           262,  7993, 46048,   286, 13624,  1626,   262, 41901,   625,   543,\n",
              "           339,   550, 11071,  1505,   764,   383,  1339,   373, 24594,   287,\n",
              "         30317, 16115,   705,    82,  4931,   379,   262, 16065,   375,   286,\n",
              "         13183,  1525,   287,   718,  2414,   837,   351,  1623,    76, 21162,\n",
              "           837,   367,   688,   290,   327,  6048, 11749,   262, 27986, 46048,\n",
              "           290,   262,  6761, 19552,   422, 22225,   272,   837,   290,  5187,\n",
              "           361,   445,  5486,   329,   262,  7993,  2292,   764,   383,  7993,\n",
              "          2728, 34429,   290,   262,  1966,  7297,   286, 21399],\n",
              "        [  796,   796, 15417,   796,   796,   220,   198, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
              "        [  383, 17119, 35622,   284, 13583, 27928,   810,   837,  1864,   284,\n",
              "           530,  4039,   705,    82,  1848,   837,   262, 17119, 16675, 40280,\n",
              "         46265,   265,  6909,   290,  5371,   683,   286, 37268,   780,   286,\n",
              "           262,   867,  7040,   837,   543,   465, 10377,   547,  4385,   284,\n",
              "           423, 13351,   764,   679, 13772,   465,  3656,   329,   748,   721,\n",
              "          8821,   465,  5536,  9007,   290,  4438,   284,  3350,   257,   649,\n",
              "          4822,   290, 11189,   326, 17119,  4219,   257,  1218,  1368,   837,\n",
              "           475,   484,  6520,   764,   220,   198, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hntWf6_50Fwg",
        "outputId": "116e1660-968b-4d5c-f923-aed4b9197bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 128])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGcdanMT0Fwg"
      },
      "source": [
        "Notice the padding: small texts are padded by `tokenizer.eos_token_id`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T79Q1UC0Fwg",
        "outputId": "fa052c6c-09b5-4042-a3d8-e0f73e273d2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS3iwc2A0Fwg"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(vocab_size=tokenizer.vocab_size, seq_len=seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFAU-IQJ0Fwg",
        "outputId": "20787ef7-8aff-4ee4-b99e-dbbe5096e301"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 128, 50257])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# same as before (only larger seq_len and V)\n",
        "logits = transformer(input_ids)\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDAOJRkg0Fwh"
      },
      "source": [
        "## Self-supervision\n",
        "\n",
        "Remember the greatest thing about Language Modeling: we don't need to annotate data!\n",
        "\n",
        "The model should predict the next word given the context so we just need to shift the input by 1 to get the labels!\n",
        "\n",
        "Compute the loss on one batch using `nn.CrossEntropyLoss`. Be careful about the padding! We don't want our model to learn to predict padding at the end of text!\n",
        "\n",
        "Like in the previous Practical Work, remember to flatten the batch dimension with the sequence dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnPSdsZ0Fwh",
        "outputId": "69c4f0cd-2b99-418e-e4e1-fcc3bc903fc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(11.0828, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loss of randomly initialized model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7laLQEx0Fwh"
      },
      "source": [
        "Notice anything about this value? What about its exponentiate? Ever heard of perplexity? More about this in the next class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD03Tkjk0Fwh"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Ensure that everything is on GPU by calling `.cuda()` or passing `device=\"cuda\"` on init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGmONLAp0Fwh"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q_cJ3JF0Fwh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(\"logs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mx-h73I0Fwh"
      },
      "source": [
        "Run tensorboard before training. Refresh during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1shnPRd-0Fwi"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcTTquTC0Fwi"
      },
      "outputs": [],
      "source": [
        "seq_len=128\n",
        "transformer = Transformer(vocab_size=tokenizer.vocab_size, hidden_size=256, num_layers=3, num_heads=4, dropout=0.1, seq_len=seq_len).cuda()\n",
        "\n",
        "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001)\n",
        "\n",
        "batch_size = 32\n",
        "# in the interest of time, we simply overfit on a single batch\n",
        "# try to train on the complete texts when you have more time\n",
        "train_loader = torch.utils.data.DataLoader(dataset[\"train\"][:batch_size], batch_size=batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "steps = 0\n",
        "for epoch in range(1000):\n",
        "    for text_batch in train_loader:\n",
        "        input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
        "        logits = transformer(input_ids)\n",
        "        raise NotImplementedError(\"TODO compute loss\")\n",
        "        writer.add_scalar(\"Loss/train\", loss.item(), steps)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "        # validation\n",
        "        if steps % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                transformer.eval()\n",
        "                valid_loss = 0\n",
        "                valid_batches = 0\n",
        "                for text_batch in validation_loader:\n",
        "                    input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
        "                    logits = transformer(input_ids)\n",
        "                    raise NotImplementedError(\"TODO compute loss\")\n",
        "                    valid_loss += loss.item()\n",
        "                    valid_batches += 1\n",
        "                transformer.train()\n",
        "                writer.add_scalar(\"Loss/validation\", valid_loss/valid_batches, steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bqtBmNc0Fwi"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIZxQsS90Fwi"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer.state_dict(), \"transformer.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPbCTCm0Fwi"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33EtAXhj0Fwi"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def decode(model, input_ids, max_new_tokens=32):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # get the predictions\n",
        "        logits = model(input_ids)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1] # becomes (B, C)\n",
        "        # greedy decoding\n",
        "        idx_next = logits.argmax(1).unsqueeze(0)\n",
        "        # append sampled index to the running sequence\n",
        "        input_ids = torch.cat((input_ids, idx_next), dim=1) # (B, T+1)\n",
        "    return input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qScTllIv0Fwj"
      },
      "outputs": [],
      "source": [
        "## load previously saved model\n",
        "#transformer.load_state_dict(torch.load(\"transformer.bin\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-DiF7F30Fwj",
        "outputId": "0102f58f-a4db-4598-ae7b-1454d8bacda5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Following the death of Finan , bishop of Lindisfarne , Alhfrith of Deira , in collusion with Wilfred of York , Agilbert of Wessex and others , were determined to persuade Oswiu to rule in favour of the Roman rite of Christianity within the kingdoms over which he had imperium . The case was debated in Oswiu 's presence at the Synod of Whitby in 664 , with Colmán , Hild and Cedd defending the Celtic rite and the tradition inherited from Aidan , and Wilifred speaking for the Roman position . The Roman cause prevailed and the former division of ecclesiastical authorities was set aside . Those who could not accept it , including Colmán , departed elsewhere . \\n\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNXTtVy0Fwj"
      },
      "source": [
        "When overfitting on one single batch, the model simply memorizes training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxca_Jj0Fwj"
      },
      "outputs": [],
      "source": [
        "prompt = \" Following\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIDV_RPG0Fwj",
        "outputId": "e9f99fed-772e-44f5-f800-05347027bd01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' Following the death of Finan, bishop of Lindisfarne, Alhfrith of Deira, in collusion with Wilfred of York, Agilbert']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids = tokenizer([prompt], return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
        "output = decode(transformer, input_ids)\n",
        "\n",
        "tokenizer.batch_decode(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhJfweDh0Fwj"
      },
      "source": [
        "It gets a bit better when you train on the 10,000 examples for 20,000 steps, but that roughly takes one hour on a labtop GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJJAThEt0Fwj",
        "outputId": "b5cbc656-5778-40ea-ad84-e147ed12a158"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' Following his home of Work Records, Shawn Conductwings days on January 2013 and Valan and 17 August 18 & 200, 1990. \\n\\n\\n\\n\\n']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiS4cr5l0Fwk",
        "outputId": "b2aa3b7c-26e7-4049-ff09-5a2a26ce6538"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\" The first first known for the United States, Texas Texas, was announced by a member of 1829 and the United States's Championship. In February 2007, the\"]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \" The\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJkvxMwC0Fwk"
      },
      "source": [
        "# Bonus: Visualize Attentions\n",
        "\n",
        "Now that we understand the basic mechanisms of attention, we can check the activated attention patterns in a pretrained BERT model (Devlin et al. 2018). Recall that BERT is an encoder-based transformer model which is based on a stack of self-attention blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI-bIuxUteuC"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from bertviz import head_view\n",
        "\n",
        "# Define a sample input text\n",
        "text = \"I will go for a run and will jump into a lake.\"\n",
        "\n",
        "# Instantiate the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Convert tokens to token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Create attention mask\n",
        "attention_mask = [1] * len(token_ids)\n",
        "\n",
        "# Convert token IDs and attention mask to tensors\n",
        "input_ids = torch.tensor([token_ids])\n",
        "attention_mask = torch.tensor([attention_mask])\n",
        "\n",
        "# Generate the transformer output\n",
        "outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
        "\n",
        "# Extract attentions and check the shape\n",
        "outputs.attentions[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPM-szhr0Fwk"
      },
      "source": [
        "As you can see, we extracted an attention from the first layer. The first dimension is the bach, the second one is the number of heads used in the first layer, and the last two dimensions are the sequence length. Given that this was a self attention block the last two numbers are equal.\n",
        "\n",
        "We can now use a method from the [bertviz library](https://github.com/jessevig/bertviz) and plot all the heads.\n",
        "\n",
        "You'll see a dropdown menu that allows you the select a layer of the model (GPT-2 has 12). You'll then see a color for every head used in that layer (GPT-2 has 12 head per layer). By default all heads are shown, click on a color to activate/disactivate that head. It can help starting by activating only one head and checking the learned relation learn by that self attentino head. By hovering over each word you can see the attention weigths that linked that words to all the others.\n",
        "\n",
        "**Question** Do you notice any interesting (linguistic) pattern?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW-PMCW40Fwk"
      },
      "outputs": [],
      "source": [
        "head_view(outputs.attentions, tokens=tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GAQAv4iil7LX"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}